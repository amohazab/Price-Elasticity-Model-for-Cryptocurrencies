{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58040e08-35fb-4f8c-b368-5ef2675461a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "#from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class MultiMinerMarket(MultiAgentEnv):\n",
    "    def __init__(self , config = None):\n",
    "        config = config or {}\n",
    "        # define reward\n",
    "        self.R = 100\n",
    "        self.marginal_cost = [1,1,1,1,1]\n",
    "        #number of agents\n",
    "        self.num_agents = len(self.marginal_cost)\n",
    "        # define a list for hash values\n",
    "        self.hashes = [0]*self.num_agents\n",
    "        # define list of probabilities\n",
    "        self.probs = [0] * self.num_agents\n",
    "        # observation_space: previous probability , previous hash\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            \"agent_\" + str(i) : gym.spaces.Box(low=np.array([0 , 0]), high=np.array([1 , self.R/self.marginal_cost[i]]), dtype=np.float32)  for i in range(0,self.num_agents)\n",
    "        })\n",
    "        \n",
    "        \n",
    "        self.action_space = gym.spaces.Dict({\n",
    "            \"agent_\" + str(i) : gym.spaces.Box(low=np.array([0]), high=np.array([self.R/self.marginal_cost[i]]), dtype=np.float32)  for i in range(0,self.num_agents)\n",
    "        })\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        #obser = self._get_obs()\n",
    "        \n",
    "        #self.marginal_valuation_vector = np.random.uniform(0,10,self.num_agents).tolist()\n",
    "        \n",
    "        #self.action_space = gym.spaces.Dict({\n",
    "         #   \"agent_\" + str(i) : gym.spaces.Box(low=np.array([0]), high=np.array([self.marginal_valuation_vector[i]]), dtype=np.float32)  for i in range(0,self.num_agents)\n",
    "        #})\n",
    "        \n",
    "        return self._get_obs()\n",
    "    \n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \n",
    "        # calculate hashes\n",
    "        hashes = []\n",
    "        for i in range(0 , self.num_agents):\n",
    "            hashes.append(action[\"agent_\" + str(i)][0])\n",
    "            \n",
    "        self.hashes = hashes\n",
    "            \n",
    "        # calculate prs as function of hashes\n",
    "        if max(hashes)==0:\n",
    "            prs = [0]*self.num_agents # exclude cases where all the chosen actions are zeros.\n",
    "        else:\n",
    "            prs = [hashes[k]/sum(hashes) for k in range(self.num_agents)] # 0<prs[i]<1  \n",
    "            \n",
    "        self.probs = prs\n",
    "            \n",
    "        # calculate rewards    \n",
    "        rew = [prs[l] * self.R - self.marginal_cost[l] * hashes[l] for l in range(self.num_agents)]\n",
    "            \n",
    "         \n",
    "        \n",
    "        rewards = {\n",
    "             \"agent_\" + str(i) : float(rew[i]) for i in range(0,self.num_agents)\n",
    "        }\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        \n",
    "        \n",
    "        is_done = True\n",
    "        \n",
    "        dones = {\n",
    "           \"agent_0\" : is_done,\n",
    "            \"agent_1\" : is_done,\n",
    "            \"agent_2\" : is_done,\n",
    "            \"agent_3\" : is_done,\n",
    "            \"agent_4\" : is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "        \n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent_\" + str(i) : [self.probs[i] ,  self.hashes[i]] for i in range(0,self.num_agents)\n",
    "        }\n",
    "\n",
    "    def render(self, mode=None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388ee52-84b1-4ae5-84ac-759ca6f1b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = MultiMinerMarket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbc76a-11ca-424e-9515-2ffd40ac3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "game = MultiMinerMarket()\n",
    "game.reset()\n",
    "count = 0\n",
    "while count < 1:\n",
    "    count = count + 1\n",
    "    action = game.action_space.sample()\n",
    "    hashes = []\n",
    "    for i in range(0 , game.num_agents):\n",
    "        hashes.append(action[\"agent_\" + str(i)][0])\n",
    "    #print(\"marginal_value\" , game.marginal_valuation_vector)\n",
    "    #print(\"bids\" , bids)\n",
    "    obs , rew , done , info = game.step(action)\n",
    "    for i in range(0 , game.num_agents):\n",
    "        print(hashes[i] , game.probs[i] , rew[\"agent_\" + str(i)])\n",
    "    game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998c466-b59a-4b20-98c7-b3d5d4331c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reset()\n",
    "a = game.observation_space\n",
    "print(a['agent_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbee3c-e1be-468a-a73a-e78b353035e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a['agent_0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec08bc-cb0f-4c97-997e-61f46845b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "\n",
    "# Start a new instance of Ray (when running this tutorial locally) or\n",
    "# connect to an already running one (when running this tutorial through Anyscale).\n",
    "\n",
    "ray.init()  # Hear the engine humming? ;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab69a2-a29e-49b6-814e-ee852efb0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "\n",
    "policies= {\n",
    "    \"policy_\" + str(i): (None , game.observation_space[\"agent_\" + str(i)] , game.action_space[\"agent_\" + str(i)] , {\"gamma\" : 0.04}) for i in range(0,game.num_agents)\n",
    "}\n",
    "\n",
    "def policy_mapping_fn (agent_id: str):\n",
    "    #assert agent_id in [str(i) for i in range(0,5)], f\"ERROR: invalid agent id {agent_id}!!!\"\n",
    "    return \"policy_\" + str(agent_id[len(agent_id)-1])\n",
    "        \n",
    "config={\n",
    "    \"env\": MultiMinerMarket,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
    "    #\"framework\": \"torch\",\n",
    "    \"model\":{\n",
    "        \"fcnet_hiddens\": [512 , 512],\n",
    "    },\n",
    "    \"num_workers\": 4,\n",
    "    \"create_env_on_driver\": True,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "},\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec3667-e3f7-4bc0-8f51-0ff7ab8322f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the DefaultCallbacks with your own and implement any methods (hooks)\n",
    "# that you need.\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self,\n",
    "                         *,\n",
    "                         worker,\n",
    "                         base_env,\n",
    "                         policies,\n",
    "                         episode: MultiAgentEpisode,\n",
    "                         env_index,\n",
    "                         **kwargs):\n",
    "        # We will use the `MultiAgentEpisode` object being passed into\n",
    "        # all episode-related callbacks. It comes with a user_data property (dict),\n",
    "        # which we can write arbitrary data into.\n",
    "\n",
    "        # At the end of an episode, we'll transfer that data into the `hist_data`, and `custom_metrics`\n",
    "        # properties to make sure our custom data is displayed in TensorBoard.\n",
    "\n",
    "        # The episode is starting:\n",
    "        # Set per-episode object to capture, which states (observations)\n",
    "        # have been visited by agent1.\n",
    "        #episode.user_data[\"market_clearing_price\"] = 0\n",
    "        # Set per-episode agent2-blocks counter (how many times has agent2 blocked agent1?).\n",
    "        #episode.user_data[\"average_accepted_bid\"] = 0\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker,\n",
    "                        base_env,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index,\n",
    "                        **kwargs):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker,\n",
    "                       base_env,\n",
    "                       policies,\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index,\n",
    "                       **kwargs):\n",
    "        # Episode is done:\n",
    "        # Write scalar values (sum over rewards) to `custom_metrics` and\n",
    "        # time-series data (rewards per time step) to `hist_data`.\n",
    "        # Both will be visible then in TensorBoard.\n",
    "        \n",
    "        #res = []\n",
    "        #for i in range(0 , game.num_agents):\n",
    "         #   if episode.last_observation_for(\"agent_\" + str(i))[1]!=0:\n",
    "          #      res.append(episode.last_observation_for(\"agent_\" + str(i))[0] - episode.last_observation_for(\"agent_\" + str(i))[1])  \n",
    "        episode.user_data[\"agent_0_hash\"] = episode.last_observation_for('agent_0')[1]\n",
    "        episode.user_data[\"agent_1_hash\"] = episode.last_observation_for('agent_1')[1]\n",
    "        episode.user_data[\"agent_2_hash\"] = episode.last_observation_for('agent_2')[1]\n",
    "        episode.user_data[\"agent_3_hash\"] = episode.last_observation_for('agent_3')[1]\n",
    "        episode.user_data[\"agent_4_hash\"] = episode.last_observation_for('agent_4')[1]\n",
    "        \n",
    "        episode.custom_metrics[\"agent_0_hash\"] = episode.user_data[\"agent_0_hash\"]\n",
    "        episode.custom_metrics[\"agent_1_hash\"] = episode.user_data[\"agent_1_hash\"]\n",
    "        episode.custom_metrics[\"agent_2_hash\"] = episode.user_data[\"agent_2_hash\"]\n",
    "        episode.custom_metrics[\"agent_3_hash\"] = episode.user_data[\"agent_3_hash\"]\n",
    "        episode.custom_metrics[\"agent_4_hash\"] = episode.user_data[\"agent_4_hash\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf1971-8923-4d2a-883b-dd44db30ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune_config = config.copy()\n",
    "tune_config[\"callbacks\"] = MyCallbacks\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop={\"training_iteration\": 100},\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=5,\n",
    "    # If you'd like to restore the tune run from an existing checkpoint file, you can do the following:\n",
    "    #restore=\"/Users/sven/ray_results/PPO/PPO_MultiAgentArena_fd451_00000_0_2021-05-25_15-13-26/checkpoint_000010/checkpoint-10\",\n",
    "    local_dir = \"5_miners_12\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad42ab7-8f32-4ca2-a0ef-e4bc250172fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amin_RL",
   "language": "python",
   "name": "amin_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
